{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b0cfe6",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6374247b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\karth\\anaconda3\\lib\\site-packages (0.26.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from gym) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym) (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b153b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "import time\n",
    "import gym\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb6ac7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the environment\n",
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd060a0a",
   "metadata": {},
   "source": [
    "# Initialzing Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5fb9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_num = env.observation_space.n\n",
    "actions_num = env.action_space.n\n",
    "\n",
    "# Q = np.zeros((states_num,actions_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa354f",
   "metadata": {},
   "source": [
    "# Fixed Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8870bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a8993d",
   "metadata": {},
   "source": [
    "# Training Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3489508b",
   "metadata": {},
   "source": [
    "#### Defining a function for updating Q_table by using episodes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0876224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_trainer(discount_rate = 0.99, no_of_episodes = 10000, learning_rate=0.01, epsilon_decay=0.001):\n",
    "    \"\"\"\n",
    "    \n",
    "    This is a function to improve our Q_table from ground level.\n",
    "    \n",
    "    Inputs = (no_of_episodes, learning_rate, epsilon_decay, Q_table)\n",
    "    \n",
    "    outputs = (Q_table , all_episodic_returns)\n",
    "\n",
    "    \"\"\"\n",
    "    Q = np.zeros((states_num,actions_num))\n",
    "    epsilon = 1\n",
    "    all_episodic_rewards = []\n",
    "    \n",
    "    for episode in range(no_of_episodes):\n",
    "        state,_ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            random_number = random.random()\n",
    "            if random_number > epsilon:\n",
    "                action = np.argmax(Q[state,:])\n",
    "            else :\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            new_state, reward,done,info,_ = env.step(action)\n",
    "            Q[state,action] = (learning_rate)*(reward + discount_rate*(np.max(Q[new_state,:]))) + (1-learning_rate)*Q[state,action]\n",
    "            state = new_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if(done==True):\n",
    "                break\n",
    "\n",
    "        all_episodic_rewards.append(episode_reward)\n",
    "        epsilon = min_epsilon + (max_epsilon-min_epsilon)*np.exp(-epsilon_decay*episode)\n",
    "    \n",
    "    return Q, all_episodic_rewards\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce9a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "Q_new, all_episodic_rewards = Q_trainer(0.99,10000, 0.01, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c1e500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_rewards_per_thousand = [0.018, 0.035, 0.036, 0.037, 0.05, 0.044, 0.043, 0.037, 0.04] \n",
      "\n",
      "Q-Table: \n",
      " [[3.58367774e-02 1.60160797e-02 1.59187759e-02 1.40467565e-02]\n",
      " [1.75957130e-04 6.69486795e-03 2.25151761e-05 6.42091352e-05]\n",
      " [2.80364023e-05 6.73226685e-03 2.98634189e-04 7.90951003e-06]\n",
      " [4.49910119e-06 1.70298368e-04 4.39705366e-07 1.66018692e-06]\n",
      " [1.73089736e-02 1.02595805e-02 3.66826067e-02 5.51842886e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.02273331e-03 7.42054585e-02 1.67931498e-03 6.99927047e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.58064609e-03 1.05917227e-02 7.44886315e-02 1.66934023e-02]\n",
      " [1.11563854e-02 1.50139889e-02 1.90180487e-01 8.68047357e-03]\n",
      " [3.85822191e-02 3.00242105e-02 2.24847496e-01 2.96377478e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [8.09150172e-03 2.50200953e-02 3.87691425e-01 1.54772749e-02]\n",
      " [3.97374852e-02 7.59882179e-02 6.19081321e-01 4.98211608e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "average_rewards_per_thousand = []\n",
    "for i in range(9):\n",
    "    sum = np.sum(all_episodic_rewards[i*1000:(i+1)*1000])\n",
    "    average_rewards_per_thousand.append(sum/1000)\n",
    "\n",
    "print('average_rewards_per_thousand =' , average_rewards_per_thousand,'\\n') \n",
    "print(\"Q-Table: \\n\", Q_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f7e832",
   "metadata": {},
   "source": [
    "# Watching the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bbc8f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = gym.make('FrozenLake-v1',render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d6c7d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n",
      "Starting a new episode\n"
     ]
    }
   ],
   "source": [
    "wins = 0\n",
    "loses = 0\n",
    "\n",
    "for epi in range(20):\n",
    "    print('Starting a new episode')\n",
    "    state = env_test.reset()[0]\n",
    "    env_test.render()\n",
    "    done  = False\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(Q_new[state,:])\n",
    "        next_state , reward, done, info, _ = env_test.step(action)\n",
    "        \n",
    "        if reward==1:\n",
    "            wins = wins+1\n",
    "            break\n",
    "            \n",
    "        elif done==True:\n",
    "            loses = loses+1\n",
    "            break  \n",
    "            \n",
    "        state = next_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01782869",
   "metadata": {},
   "source": [
    "# Results and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccec6519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of times our agent won out of 20 times: 2\n",
      "no of times our agent lost out of 20 times: 18\n",
      "no of times our agent draw out of 20 times: 0\n"
     ]
    }
   ],
   "source": [
    "print('no of times our agent won out of 20 times:', int(wins) )\n",
    "print('no of times our agent lost out of 20 times:', int(loses) )\n",
    "print('no of times our agent draw out of 20 times:', int(20-wins-loses) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d9ff20",
   "metadata": {},
   "source": [
    "# Improving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b14cc602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function Q_trainer in module __main__:\n",
      "\n",
      "Q_trainer(discount_rate=0.99, no_of_episodes=10000, learning_rate=0.01, epsilon_decay=0.001)\n",
      "    This is a function to improve our Q_table from ground level.\n",
      "    \n",
      "    Inputs = (no_of_episodes, learning_rate, epsilon_decay, Q_table)\n",
      "    \n",
      "    outputs = (Q_table , all_episodic_returns)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need epsilon decay and learning rate patterns\n",
    "help(Q_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f045e57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for lr=0.5, decay=0.1 is 0.0\n",
      "accuracy for lr=0.5, decay=0.01 is 0.0\n",
      "accuracy for lr=0.5, decay=0.001 is 586.0\n",
      "accuracy for lr=0.1, decay=0.1 is 0.0\n",
      "accuracy for lr=0.1, decay=0.01 is 675.0\n",
      "accuracy for lr=0.1, decay=0.001 is 693.0\n",
      "accuracy for lr=0.01, decay=0.1 is 0.0\n",
      "accuracy for lr=0.01, decay=0.01 is 113.0\n",
      "accuracy for lr=0.01, decay=0.001 is 48.0\n",
      "accuracy for lr=0.001, decay=0.1 is 0.0\n",
      "accuracy for lr=0.001, decay=0.01 is 0.0\n",
      "accuracy for lr=0.001, decay=0.001 is 59.0\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.5,0.1,0.01,0.001]\n",
    "epsilon_decays = [0.1,0.01,0.001]\n",
    "accuracy = []\n",
    "\n",
    "for i in learning_rates:\n",
    "    for j in epsilon_decays:\n",
    "        Q_improve,Rewards = Q_trainer(0.99,10000,i,j)\n",
    "        r = np.sum(Rewards[9000:10000])\n",
    "        accuracy.append(r)\n",
    "        print(f'accuracy for lr={i}, decay={j} is {r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6672473f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for dr=0.99, decay=0.001 is 0.0\n",
      "accuracy for dr=0.9, decay=0.001 is 0.0\n",
      "accuracy for dr=0.8, decay=0.001 is 0.0\n",
      "accuracy for dr=0.7, decay=0.001 is 0.0\n"
     ]
    }
   ],
   "source": [
    "discount_rates = [0.99,0.9,0.8,0.7]\n",
    "for i in discount_rates:\n",
    "    Q_improve,Rewards = Q_trainer(i,10000,0.1,0.01)\n",
    "    r = np.sum(Rewards[9000:10000])\n",
    "    accuracy.append(r)\n",
    "    print(f'accuracy for dr={i}, decay={j} is {r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43469136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
